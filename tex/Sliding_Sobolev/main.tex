\documentclass{beamer}

% Set up
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{mathrsfs}  
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
% \usepackage[utf8]{inputenc}

% Beamer setup
\usetheme{default}
\usecolortheme{default}
% \AtBeginSection[]
% {
%   \begin{frame}
%     \frametitle{Table of Contents}
%     \tableofcontents[currentsection]
%   \end{frame}
% }
\usefonttheme[onlymath]{serif}

% Math commands
\input{math}



%Information to be included in the title page:
\title[Inequalities for Markov chains with non-neg Ricci]{Poincaré, modified logarithmic Sobolev and
isoperimetric inequalities for Markov chains with
non-negative Ricci curvature \\~\\ \small M. Erbar and M. Fathi} 
\author[Matilde Dolfato]{\small Matilde Dolfato \\ Mentor: Prof. Elia Brué}
\institute[]{Università Bocconi  \par Visiting Student Initiative - BIDSA}
\date{\today}

\begin{document}

\frame{\titlepage}
% % comment on title

\begin{frame}{Overview of the talk}
    \tableofcontents
\end{frame}
\section{Introduction}

\begin{frame}
    \centering
    We present the paper \\
    {\color{blue}\textit{Poincaré, modified logarithmic Sobolev and
    isoperimetric inequalities for Markov chains with
    non-negative Ricci curvature}}\\
    as a follow up of \\

    \textit{Ricci curvature of finite markov chains via convexity of the entropy}
\pause
\\~\\
    \begin{enumerate}
        \item The main goal (of the first paper) is to introduce a new geometric structure in the study of Markov semigroups (Markov chains)
        \pause
        \item This is done via an interpratation of the semigroup as a gradient flow of the Shannon entropy
        \item Leading to a synthetic notion of Ricci curvature to study contraction properties
    \end{enumerate}
\pause
    {\color{blue} New ingredients (content of the new paper):} functional inequalities and applications to bounds on the mixing time of the chain and to its convergence to equilibrium
\end{frame}

\begin{frame}
    \begin{itemize}
        \item Geometric structure on space of Markov chains
        \item Synthetic notion of Ricci curvature ($\Ric \ge 0$)
        \item Analogous version of functional inequalities for Markov chains
        \item Bounds on mixing time and rate of convergence to equilibrium (among others)
    \end{itemize}
\end{frame}


\section{Main result: convergence to equilibrium of the zero-range process}

\subsection{The zero-range process}
\begin{frame}[allowframebreaks]{The zero-range process}
    Consider $K$ interacting particles on the complete graph with $L$ sites.\\~\\

    The state space is $\X_{K,L}=\{\eta \in \mathbb{N}^L: \sum_i \eta_i = K\}$. \\~\\

    The zero-range process with \textbf{constant rates} can be described as:
    \begin{enumerate}
        \item Choose a site $i$ uniformly at random
        \begin{enumerate}
            \item If $\eta_i=0$, do nothing
            \item Else, choose another site $j$ uniformly at random and move 1 particle from $i$ to $j$
        \end{enumerate}
    \end{enumerate}
    $\eta^{i,j}$ denotes the new configuration after such a move.

    \begin{equation*}
    Q_{K,L}(\eta,\theta)=
    \begin{cases}
        \frac{1}{L} & \theta=\eta^{i,j} \text{ for some }i, j\\
        0 & \text{else}
    \end{cases}
    \end{equation*}
    Invariant measure is the uniform measure $\pi_{K,L}$.
\\~\\
Good model for:
\begin{itemize}
    \item traffic flow
    \item population dynamics
    \item particle physics
    \item ... any system in which deciding to move depends on the surroundings


    {\color{blue}EB: se trovi il nome specifico di modelli in traffico flow, population design, etc. aggiungili}
\end{itemize}
\end{frame}

\subsection{Bound on mixing time and interpretation}
\begin{frame}{Main result and interpretation}
    % HIGH LEVEL dont even tell them it's called log Sobolev or just mention
    \begin{block}{Convergence to equilibrium}
        The constant-rate zero range process with $K$ particles and $L$ sites has mixing time bounded by
        \begin{equation}
        \tau_{\rm mix}(\varepsilon) \le K L \log L \bigg(\frac{1}{8} - \frac{\log \varepsilon}{c}\bigg)
        \end{equation}
        for some universal constant $c$. 


% {\color{blue}EB: se non scrive altro nella slide (e va benissimo) dovi spendere molto tempo per spiegare la notazione a parole (o magari usando la lavagna). In particolare dovresti spiegare cos'è $\tau_{\rm mix}$, qual'è il ruolo di $\varepsilon$ in questa definizione, cosa vuol dire in termini del processo.}

        
    \end{block}
\pause
\begin{center}{\color{blue} $\star$ This is exponential convergence to equilibrium! $\star$}\end{center}
\end{frame}


\section{From geometry to inequalities}
\tableofcontents[currentsection]
\subsection{Setup}
\begin{frame}{Setup}
    Finite space of states $\X$, irreducible, reversible Markov kernel $Q$ and stationary measure $\pi$
  \[
   Q(x,y)\pi(x)=Q(y,x)\pi(y)
  \]
  Operator $L$ acting on functions $\psi:\X \to\R$ is\[L\psi(x)=\sum_{y\in\X}(\psi(y)-\psi(x))Q(x,y)\]
  is the generator of a continuous time Markov chain.

 Space of probability densities
  \[
    \mP(\X)
    := \Bigl\{\rho:\X\to\R_+ : \sum_{x\in\X} \rho(x)\,\pi(x) = 1\Bigr\}
  \]
\end{frame}
\begin{frame}{Distance $\W$}
    We define a discrete transport distance, by analogy with the Wasserstein distance. For $\rho_0,\rho_1 \in \mP(\X),$
    \[
\W(\rho_0, \rho_1)^2 := 
\inf_{\rho, \psi} 
\; \frac{1}{2} 
\int_0^1 \sum_{x,y \in X} 
\bigl( \psi_t(x) - \psi_t(y) \bigr)^2 
\, \hat{\rho}_t(x,y) \, Q(x,y) \, \pi(x) 
\, dt.
\]
where the infimum runs over all sufficiently regular curves satisfying a continuity equation
\[
\begin{cases}
\displaystyle
\frac{d}{dt}\rho_t(x) 
+ \sum_{y \in \mathcal{X}} 
\bigl( \psi_t(y) - \psi_t(x) \bigr)
\hat{\rho}_t(x,y) Q(x,y) 
= 0, 
& \forall x \in \mathcal{X}, \\[1.2em]
\rho|_{t=0} = \rho_0, \quad 
\rho|_{t=1} = \rho_1.
\end{cases}
\]
and $\hat\rho(x,y):=\theta(\rho(x),\rho(y))$ which is the logarithmic mean.
\\~\\
\centering
Endowing $\X$ with this distance, we interpret the Markov semigroup $P_t=e^{tL}$ as a gradient flow of Shannon's entropy $\mathcal{H}(\rho):=\sum_{x\in\X} \rho(x) \log \rho(x) \pi(x)$
\end{frame}
\subsection{Ricci for discrete processes}

\begin{frame}{Ricci for discrete processes}
    \begin{block}{Entropic Ricci curvature for discrete space}
    $(\X,Q,\pi)$ has entropic Ricci curvature bounded from below by $\kappa \in \R$ if for any geodesic \{$\rho_t\}_{t\in[0,1]}$ on $ (\mP(\X), \W)$ 
    %EB: intendi una geodetica rispetto a quella struttrura metrica? Non userei questa notazione 
    we have
    \[
    \mathcal{H}(\rho_t) \le (1-t)\mathcal{H}(\rho_0) + t\mathcal{H}(\rho_1)- \frac{\kappa}{2}t(1-t)\W(\rho_0,\rho_1)^2
    \]
    In this case, we write $\Ric(\X,Q,\pi)\ge \kappa$.
    \end{block}
\end{frame}
\begin{frame}{Ricci for the zero-range process}
    In another contribution, M. Fathi and J. Maas provide explicit ways of computing bounds on Ricci for discrete processes
    \\~\\
    They show that for the ZRP with \textit{increasing} rates, Ricci is positive
    \\~\\
    The ZRP with \textit{constant} rates has $\Ric \ge 0$
\end{frame}

\subsection{Modified Logarithmic Sobolev inequality for discrete processes with non-neg Ricci}
\begin{frame}[allowframebreaks]{The Modified log-Sobolev inequality}
\begin{block}{Modified logarithmic Sobolev inequality}
Bound on the entropy in terms of a norm of the gradient of our observable:
    \begin{equation*}\tag{MLSI}
        \mathcal{H}(\rho) \le \frac{1}{2\lambda} \int \rho | \nabla \log \rho |^2 \de \pi = \frac{1}{2\lambda} \mathcal{I}(\rho)
    \end{equation*}
where $\mathcal{I}$ is the Fisher information.
\end{block}
\pause
Implications:
\begin{itemize}
    \item encodes the distribution of the eigenvalues of the Laplacian (generator $L$)
    \item entropy decay along the heat flow \(\mathcal{H}(P_t \rho) \le e^{-2\lambda t}\mathcal{H}(\rho)\)
    \item {\color{blue}\textbf{convergence to equilibrium}}
    \item convergence in Wassertsein distance \(\W(\rho_t, \sigma_t) \le e^{-2\lambda t} \W(\rho_0, \sigma_0)\)
    \item hypercontractivity \(\lVert P_t \rho\rVert\le \lVert\rho \rVert\)
\end{itemize}

\end{frame}

\begin{frame}{From MLSI to entropy decay}
    Interpreting the markov semigroup $P_t$ as the heat semigroup, we get that fisher information is the derivative of the entropy. 
    
    Hence
    \begin{equation*}\tag{MLSI}
        \mathcal{H}(P_t\rho) \le \frac{1}{2\lambda} \frac{\de}{\de t}\mathcal{H}(P_t \rho)
    \end{equation*}

    Using Gronwall's inequality, this yields an estimate on entropy decay
    \begin{equation*}
        \mathcal{H}(P_t \rho) \le e^{-2\lambda t} \mathcal{H}(\rho)
    \end{equation*}
    that quantifies {\color{blue} convergence to equilibrium}. 
\end{frame}

\begin{frame}{From entropy decay to bound on mixing time}
We look at convergence to equilibrium in terms of the \textbf{mixing time} of the Markov chain. 

\begin{block}{Total variation mixing time of a Markov chain}
    For $\varepsilon > 0$, 
    \[
    \tau_{\rm mix}(\varepsilon) := \inf \Big\{t>0 : \lVert P_t^* \delta_x - \pi \rVert_{TV} < \varepsilon \quad \forall x \in \X \Big\}
    \]
\end{block}

Since
\begin{equation*}\tag{Pinsker inequality}
\lVert \nu - \pi \rVert^2_{TV} \le \frac{1}{2} \mathcal{H}(\nu)
\end{equation*}
we can use MLSI to bound $\tau_{\rm mix}$.

\begin{block}{Convergence to equilibrium}
    Assume $\Ric(\X, Q, \pi)\ge 0$ and the diameter of $(\X,d_{\W})$ is $\le D$. If a MLSI with constant $\lambda= \frac{c}{D^2}$ holds, then
    \[
    \tau_{mix}(\varepsilon) \le D^2(1/8-c\log\varepsilon)    \]
    for some universal constant $c$.
\end{block}


{\color{blue} EB: E' necessario chiarire come tutti questi concetti e queste disuguaglianze si combinano per ottenere la stima sul tempo di mixing. 

(1) Cosa centra MLSI con il fatto che il semigruppo è il flusso dell'entropia? Questo punto è talmente fondamentale che potresti considerare di far vedere il calcolo per ottenere entropy decay

(2) Capito (1), come si combina entropy decay e Pinsker per concludere? Anche questo è esplicitabile.


}




\end{frame}

\begin{frame}{Modified Log-Sobolev inequality for the zero-range process}
    \begin{block}{Diameter bound}
        There is a constant $c>0$ such that the diameter of $(\X_{K,L}, d_\W)$ is bounded by $cK\sqrt{L\log L}$
    \end{block}

    So, for the zero range process with constant uniform rates, a MLSI with constant $\frac{c}{K^2 L \log L}$ holds. 

    \begin{block}{Convergence of the ZRP}
        The total variation mixing time of the zero range process on $\X_{K,L}$ with constant uniform rates is 
        \[
        \tau_{\rm mix}(\varepsilon) \le K L \log L \bigg(\frac{1}{8} - \frac{\log \varepsilon}{c}\bigg)
        \]
        for some universal constant $c$.
    \end{block}
\end{frame}  


% \section{Further inequalities}
% \tableofcontents[currentsection]
% \subsection{Poincaré inequality and the spectral gap}
% \begin{frame}{Poincaré inequality}
%     % some facts on spectral gap (see notes on log sobolev)
% \end{frame}
% \subsection{Isoperimetric inequality and Cheeger constant}
% \begin{frame}{Isoperimetric inequality and Cheeger constant}
%     % notes from prof
% \end{frame}

% \section{Other applications}
% \begin{frame}{example 1}
%     % notes from prof
% \end{frame}
% \begin{frame}{example 2}
    
% \end{frame}

\begin{frame}[allowframebreaks]{Other examples}
    lalaland
\end{frame}

\begin{frame}{Additional formulas}
    The distance $\W$ on $\mP(\X)$ induces a distance on $\X$ by restricting to Dirac masses, i.e. for $x,y\in\X$
    \[d_\W(x,y) := \W(\delta_x, \delta_y)\]
\end{frame}

% other examples of what this theoreitical framework allows us to get     

\end{document}
