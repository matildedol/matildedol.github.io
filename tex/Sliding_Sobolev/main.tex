\documentclass{beamer}

% Set up
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{mathrsfs}  
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
% \usepackage[utf8]{inputenc}

% Beamer setup
\usetheme{default}
\usecolortheme{default}
% \AtBeginSection[]
% {
%   \begin{frame}
%     \frametitle{Table of Contents}
%     \tableofcontents[currentsection]
%   \end{frame}
% }
\usefonttheme[onlymath]{serif}

% Math commands
\input{math}

%Title page:
\title[Inequalities for Markov chains with non-neg Ricci]{Poincaré, modified logarithmic Sobolev and
isoperimetric inequalities for Markov chains with
non-negative Ricci curvature \\~\\ \small M. Erbar and M. Fathi, 2016} 
\author[Matilde Dolfato]{\small Matilde Dolfato \\ Mentor: Prof. Elia Brué}
\institute[]{Università Bocconi  \par Visiting Student Initiative - BIDSA}
\date{\today}

\begin{document}

\frame{\titlepage}
% % comment on title

\section{Introduction}
\begin{frame}
    \centering
    We present the paper \\
    {\color{blue}\textit{Poincaré, modified logarithmic Sobolev and
    isoperimetric inequalities for Markov chains with
    non-negative Ricci curvature}}\\
    as a follow up of \\

    \textit{Ricci curvature of finite markov chains via convexity of the entropy}
\pause
\\~\\
    \begin{enumerate}
        \item The main goal (of the first paper) is to introduce a new geometric structure in the study of Markov semigroups (Markov chains)
        \pause
        \item This is done via an interpratation of the semigroup as a gradient flow of the Shannon entropy w.r.t. a newly defined metric structure
        
        %Magari di a paroile che la struttura metrica è ispirata da wasserstein e la teoria nel constesto continuo
        \item Leading to a synthetic notion of Ricci curvature {\color{blue}for discrete spaces} to study contraction properties
    \end{enumerate}
\pause
    {\color{blue} New ingredients (content of the new paper):} functional inequalities and applications to bounds on the mixing time of the chain and to its convergence to equilibrium
\end{frame}

\begin{frame}{Road from old paper to present}
    \begin{itemize}
        \item Geometric structure on space of Markov chains
        \item[]
        \item Synthetic notion of Ricci curvature ($\Ric \ge 0$)
        \item[] 
        \item Analogous version of functional inequalities for Markov chains
        \item[] 
        \item Bounds on mixing time and rate of convergence to equilibrium (among others)
    \end{itemize}
\end{frame}

\section{Convergence to equilibrium of the zero-range process}
\begin{frame}{Overview of the talk}
    \tableofcontents[currentsection]
\end{frame}
\subsection{The zero-range process}
\begin{frame}[allowframebreaks]{The zero-range process}
    Consider $K$ interacting particles on the complete graph with $L$ sites\\~\\

    The state space is $\X_{K,L}=\{\eta \in \mathbb{N}^L: \sum_i \eta_i = K\}$ \\~\\

    The zero-range process with \textbf{constant rates} can be described as:
    \begin{enumerate}
        \item Choose a site $i$ uniformly at random
        \begin{enumerate}
            \item If $\eta_i=0$, do nothing
            \item Else, choose another site $j$ uniformly at random and move 1 particle from $i$ to $j$
        \end{enumerate}
    \end{enumerate}
    $\eta^{i,j}$ denotes the new configuration after such a move\\
For $\eta,\theta \in \X_{K,L},$
    \begin{equation*}
    Q_{K,L}(\eta,\theta)=
    \begin{cases}
        \frac{1}{L} & \theta=\eta^{i,j} \text{ for some }i, j\\
        0 & \text{else}
    \end{cases}
    \end{equation*}
    Stationary measure $\pi_{K,L}$ is the uniform measure \\

\[\]
Good model for:
\begin{itemize}
    \item Traffic-jam formation~\cite{trafficflow}
    \item Population dynamics~\cite{populationnetwork}
    \item Rewiring networks (e.g. Bianconi–Barabási model)~\cite{populationnetwork},~\cite{networks}
    \item $\ldots$ any system in which deciding to move depends on the surroundings
\end{itemize}
\end{frame}

\subsection{Bound on mixing time and interpretation}
\begin{frame}{Main result and interpretation}
    % HIGH LEVEL dont even tell them it's called log Sobolev or just mention
    \begin{block}{Convergence to equilibrium}
        The constant-rate zero range process with $K$ particles and $L$ sites has mixing time bounded by
        \begin{equation}
        \tau_{\rm mix}(\varepsilon) \le K^2 L \log L \bigg(\frac{1}{8} - c\log \varepsilon\bigg)
        \end{equation}
        for some universal constant $c$ 


% {\color{blue}EB: se non scrive altro nella slide (e va benissimo) dovi spendere molto tempo per spiegare la notazione a parole (o magari usando la lavagna). In particolare dovresti spiegare cos'è $\tau_{\rm mix}$, qual'è il ruolo di $\varepsilon$ in questa definizione, cosa vuol dire in termini del processo.}

        
    \end{block}
    \bigskip
    \centering{\color{blue} $\star$ This is exponential convergence to equilibrium! $\star$}
\end{frame}

\section{Geometric structure}
\tableofcontents[currentsection]
\subsection{Setup}
\begin{frame}{Setup}
    Finite space of states $\X$, irreducible, reversible Markov kernel $Q$ and stationary measure $\pi$
  \[
   Q(x,y)\pi(x)=Q(y,x)\pi(y)
  \]
  Operator $L$ acting on functions $\psi:\X \to\R$ is\[L\psi(x)=\sum_{y\in\X}(\psi(y)-\psi(x))Q(x,y)\]
  is the generator of a continuous time Markov chain

 Space of probability densities
  \[
    \mP(\X)
    := \Bigl\{\rho:\X\to\R_+ : \sum_{x\in\X} \rho(x)\,\pi(x) = 1\Bigr\}
  \]
\end{frame}

\subsection{Distance $\W$}
\begin{frame}{Distance $\W$}
    We define a discrete transport distance, by analogy with the Wasserstein distance. For $\rho_0,\rho_1 \in \mP(\X)$,
    \begin{equation*}
    \W{(\rho_0, \rho_1)}^2 := \inf_{\rho, \psi} 
    \; \frac{1}{2} 
    \int_0^1 \sum_{x,y \in X} 
    {(\psi_t(x) - \psi_t(y))}^2 
    \, \hat{\rho}_t(x,y) \, Q(x,y) \, \pi(x) 
    \, dt
    \end{equation*}
    where the infimum runs over all sufficiently regular curves satisfying a continuity equation
    \begin{equation*}
    \begin{cases}
    \displaystyle
    \frac{d}{dt}\rho_t(x) 
    + \sum_{y \in \mathcal{X}} 
    \bigl( \psi_t(y) - \psi_t(x) \bigr)
    \hat{\rho}_t(x,y) Q(x,y) 
    = 0, 
    & \forall x \in \mathcal{X}, \\
    \rho|_{t=0} = \rho_0, \quad 
    \rho|_{t=1} = \rho_1
    \end{cases}
    \end{equation*}
    and $\hat\rho(x,y):=\theta(\rho(x),\rho(y))$ is the logarithmic mean
\end{frame}

\begin{frame}{Markov semigroup as gradient flow}
{\small    \begin{equation*}
        \W{(\rho_0, \rho_1)}^2 := \inf_{\rho, \psi} 
        \; \frac{1}{2} 
        \int_0^1 \sum_{x,y \in X} 
        {(\psi_t(x) - \psi_t(y))}^2 
        \, \hat{\rho}_t(x,y) \, Q(x,y) \, \pi(x) 
        \, dt
        \end{equation*}
    \begin{equation*}
        {\rm s.t.} \quad
        \frac{d}{dt}\rho_t(x) 
        + \nabladot (\hat{\rho}(x,y)\nabla \psi(x,y)) =0
    \end{equation*}}
\centering
    Endowing $\X$ with this distance, we can interpret the Markov semigroup $P_t=e^{tL}$ as a gradient flow of Shannon's entropy \begin{equation*}
        \mathcal{H}(\rho):=\sum_{x\in\X} \rho(x) \log \rho(x) \pi(x)
    \end{equation*}
\end{frame}


\subsection{Ricci curvature for discrete processes}

\begin{frame}{Ricci for discrete processes}
    \begin{block}{Entropic Ricci curvature for discrete space}
    $(\X,Q,\pi)$ has entropic Ricci curvature bounded from below by $\kappa \in \R$ if for any geodesic ${\{\rho_t\}}_{t\in[0,1]}$ on $ (\mP(\X), \W)$ 
    %EB: intendi una geodetica rispetto a quella struttrura metrica? Non userei questa notazione 
    we have
    \[
    \mathcal{H}(\rho_t) \le (1-t)\mathcal{H}(\rho_0) + t\mathcal{H}(\rho_1)- \frac{\kappa}{2}t(1-t)\W{(\rho_0,\rho_1)}^2
    \]
    In this case, we write $\Ric(\X,Q,\pi)\ge \kappa$
    \end{block}
\end{frame}
\begin{frame}{Ricci for the zero-range process (in brief)}
\begin{center}
    {\color{blue} The zero range process with constant rates has $\Ric \ge 0$} \end{center}
    In another contribution, M. Fathi and J. Maas provide explicit ways of computing bounds on Ricci for discrete processes
    \\~\\
    They show that for the ZRP with \textit{increasing} rates, Ricci is positive
    \\~\\
    This easily extends to showing non-negative Ricci for \textit{constant} rates
\end{frame}

\section{Modified log-Sobolev inequality}

\begin{frame}{Overview of the talk}
    \tableofcontents[currentsection]
\end{frame}

\begin{frame}{The modified log-Sobolev inequality I}
\begin{block}{Modified logarithmic Sobolev inequality}
It is a \textbf{bound on the entropy} in terms of a norm of the gradient of our observable:
    \begin{equation}\tag{mLSI($\lambda$)}\label{eq: mLSI}
        \mathcal{H}(\rho) \le \frac{1}{2\lambda} \lVert \nabla \log \rho \rVert_\rho ^2
    \end{equation}
where $\lVert \nabla \log \rho \rVert_\rho ^2 \eqqcolon \mathcal{I}(\rho)$ is the Fisher information % $\mathcal{I}(\rho):= \int \rho | \nabla \log \rho |^2 \de \pi = \lVert \nabla \log \rho \rVert_\rho ^2$ %{\color{blue}EB: displayed?}
\end{block}
\pause[]
\bigskip
\begin{block}{mLSI for $\Ric \ge 0$}
If $\Ric(\X,Q,\pi)\ge 0$ and the diameter of $(\X,d_\W)$ is bounded
by $D$, then the modified logarithmic Sobolev inequality (\ref{eq: mLSI}) holds with constant \[\lambda=\frac{c}{D^2}\] for some universal constant $c$
\end{block}
\end{frame}

\begin{frame}{The modified log-Sobolev inequality II}
    The mLSI has many implications on convergence property, spectrum of the Laplacian and hypercontractivity
\\
\bigskip
\centering
{\color{blue}  $\star$ Our focus: mLSI implies entropy decay, which allows to bound the mixing time $\star$ }
\begin{equation*}\tag{Entropy decay}
    \mathcal{H} (P_t \rho) \le e^{-2\lambda t} \mathcal{H}(\rho)
\end{equation*}
\end{frame}
\section{From mLSI to convergence to equilibrium}
\begin{frame}{Overview of the talk}
    \tableofcontents[currentsection]
\end{frame}
\begin{frame}{From mLSI to entropy decay}
{   \small
    \begin{equation*}
        \mathcal{H}(\rho) \le \frac{1}{2\lambda} \mathcal{I}(\rho)\quad \rightarrow\quad \mathcal{H} (P_t \rho) \le e^{-2\lambda t} \mathcal{H}(\rho)
    \end{equation*}}
    When $P_t$ is the heat semigroup,
    \begin{equation*}
        \frac{\de}{\de t} \mathcal{H}(P_t \rho) = -\mathcal{I}(P_t \rho)
        {\color{blue} \overset{{\rm mLSI}}{\le} -2\lambda \mathcal{H}(P_t\rho)}
    \end{equation*}

% {\color{blue}EB: di a parole che questo è il punto in cui tutta la struttura entra in gioco: (1) $P_t \rho$ è gradient flow di entropy, quindi time derivative=gradient of entropy. (2) In the W structure, gradient of entropy is Fisher

% \\

% Puoi chiudere la disuguaglianza in una sola riga invece che in due come ho fatto sopra (magari correggi l'errore che ho causato :))}
\pause
    Using Gronwall's inequality, this yields an estimate on entropy decay
    \begin{equation*}
        \mathcal{H}(P_t \rho) \le e^{-2\lambda t} \mathcal{H}(\rho)
    \end{equation*}
    that quantifies {\color{blue} convergence to equilibrium}
\end{frame}


\begin{frame}{From entropy decay to bound on mixing time}
{   \small
    \begin{equation*}
        \mathcal{H} (P_t \rho) \le e^{-2\lambda t} \mathcal{H}(\rho) \quad \rightarrow
        \quad \tau_{\rm mix} \le \frac{1}{8} D^2 -\frac{\log\varepsilon}{\lambda}
    \end{equation*}}

   \begin{block}{Total variation mixing time of a Markov chain}
    Let $P_t$ be the Markov semigroup on the space of densities $\mP(\X)$.
    For $\varepsilon > 0$, 
    \[
    \tau_{\rm mix}(\varepsilon) := \inf \Big\{t>0 : \lVert P_t \rho - 1 \rVert_{TV} < \varepsilon \quad \forall x \in \X \Big\}
    \]
    \end{block}
\pause
    \begin{block}{$\rightarrow$ Relation to entropy: Pinsker's inequality}
        Let $\rho, \sigma$ be two probability densities over $\X$. Then
        \begin{equation*}\tag{Pinsker}
            \lVert \rho -\sigma \rVert_{TV}^2 \le \frac{1}{2} \mathcal{H(\rho | \sigma)}
        \end{equation*}
    \end{block}

\end{frame}

\begin{frame}[allowframebreaks]{From entropy decay to bound on mixing time}
    {\small Ingredients:
    \begin{enumerate}
        \item Bound on $\mathcal{H}$ in terms of the diameter: $\mathcal{H}(P_t \rho) \le \frac{\W^2(\rho, 1)}{4t}$
        \item Pinsker's: $\lVert P_t\rho - 1 \rVert_{TV}^2 \le \frac{1}{2} \mathcal{H}(P_t\rho)$
        \item Entropy decay: $\mathcal{H}(P_t \rho) \le e^{-2\lambda t} \mathcal{H}(\rho)$
    \end{enumerate}}

    From (1) we obtain $\mathcal{H}(P_t \rho) \le 2$ for 
    \begin{equation}\tag{bound \#1}
        t \ge \frac{D^2}{8} \quad\coloneq t_0
    \end{equation}

    Plug this into (2) and combine with (3) to obtain
    \begin{equation*}
        {\lVert P_t \rho - 1 \rVert}_{TV} \le \sqrt{\frac{1}{2}\, \mathcal{H}(P_t\rho)} \le \sqrt{\frac{1}{2} \,e^{-2\lambda t} \,\mathcal{H}(P_{t_0}\rho)} \le e^{-\lambda t}
    \end{equation*}

    We get $\varepsilon$-closeness for
    \begin{equation}\tag{bound \#2}
        t \ge  -\frac{\log\varepsilon}{\lambda}
    \end{equation}

    Sum the two bounds $t\ge D^2/8$ and $t\ge -\log\varepsilon/\lambda$ to obtain an upper bound on the mixing time
    \begin{equation*}
        \tau_{\rm mix} \le \frac{1}{8} D^2 -\frac{\log\varepsilon}{\lambda}
    \end{equation*}

\end{frame}

\begin{frame}{Bound on the mixing time of the zero-range process}
    {\small     \begin{equation*}
        \tau_{\rm mix} \le \frac{1}{8} D^2 -\frac{\log\varepsilon}{\lambda}
    \end{equation*}
    }
    {\color{blue} Recall:} The ZRP has $\Ric(\X_{KL},Q_{KL},\pi_{KL})\ge 0$, hence a mLSI with constant $\lambda=\frac{c}{D^2}$ holds
\\~\\
    {\color{blue} Moreover:} There exists a constant $c > 0$ such that for any $L, K$ \[D_{KL} \le c K\sqrt{L\log L}\]

    % Plugging this into the bound, we get a bound for the ZRP mixing time 
    \begin{block}{Convergence of the ZRP}
        The total variation mixing time of the zero range process on $\X_{K,L}$ with constant uniform rates is 
        \[
        \tau_{\rm mix}(\varepsilon) \le K^2 L \log L \bigg(\frac{1}{8} - c\log \varepsilon\bigg)
        \]
        for some universal constant $c$
    \end{block}
\end{frame}  

\section{References}
\begin{frame}[allowframebreaks]{References}
    \bibliographystyle{apalike}
    \bibliography{refs}
\end{frame}


% \section{Further inequalities}
% \tableofcontents[currentsection]
% \subsection{Poincaré inequality and the spectral gap}
% \begin{frame}{Poincaré inequality}
%     % some facts on spectral gap (see notes on log sobolev)
% \end{frame}
% \subsection{Isoperimetric inequality and Cheeger constant}
% \begin{frame}{Isoperimetric inequality and Cheeger constant}
%     % notes from prof
% \end{frame}

% \section{Other applications}
% \begin{frame}{example 1}
%     % notes from prof
% \end{frame}
% \begin{frame}{example 2}
    
% \end{frame}

\begin{frame}{Discussion}
\begin{itemize}
    \item The main contribution of this paper for the ZRP is a \textit{quantification} of 
    its convergence (~\cite{ANDJEL200067})
    \item[]
    \item The dependence on the diameter is optimal, since it is sharp (up
    to the values of the constant) for the random walk on the one-dimensional
    discrete torus
    \item However, the mLSI constant for ZRP is not believed to be optimal: dependence
    of curvature on $D$ in high dimensions is not optimal (Gaussian concentration instead)
    \item Developments in this sense:~\cite{münch2023olliviercurvatureisoperimetryconcentration},~\cite{brannan2020completelogarithmicsobolevinequalities}
\end{itemize}
\end{frame}
\begin{frame}{Some examples of other applications}
    mLSI constant (and Poincaré) and implied convergence rate for:
    \begin{itemize}
        \item Bernoulli-Laplace model (diffusion of two incompressible gases)
        \item Random transposition model (random permutations)~\cite{caputo2007convexentropydecaybochnerbakryemery}
        \item Hard-core models (nearest-neighbour exclusion)
        \item Glauber dynamics (Ising model)~\cite{glauber}
    \end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]{Additional formulas}
    \begin{block}{Distance $d_\W$ on $\X$}
        The distance $\W$ on $\mP(\X)$ induces a distance on $\X$ by restricting to 
        Dirac masses, i.e. for $x,y\in\X$
        \begin{equation*}
            d_\W(x,y) := \W(\delta_x, \delta_y)
        \end{equation*}
    \end{block}

    \begin{block}{Induced Riemannian structure on $\X$}
        \begin{equation*}
            {(\Psi,\Phi)}_\rho:=\frac{1}{2}\sum_{x,y}\Psi(x,y)\Phi(x,y)\hat{\rho}
            (x,y)Q(x,y)\pi(x)
        \end{equation*}
    \end{block}

    \begin{block}{Discrete gradient and divergence}
        \begin{align*}
            \nabla \psi(x,y) &= \psi(y) - \psi(x)\\[4pt]
            \nabladot (\nabla \psi)(x) &= \tfrac{1}{2}\sum_y \big(\nabla\psi(x,y) - \nabla\psi(y,x)\big) Q(x,y)\\
            &= \tfrac{1}{2}\sum_y \big(\psi(y) - \psi(x) - \psi(x) + \psi(y)\big) Q(x,y)\\
            &= \sum_y (\psi(y)-\psi(x)) Q(x,y)
        \end{align*}
    \end{block}
    \begin{block}{Heat flow as gradient flow of entropy}
        Heat equation $\rho'_t=\Delta \rho_t = \nabladot \nabla \rho_t$\\
        Continuity equation $\rho'_t + \nabladot(\hat{\rho}\nabla\psi)=0$\\
        The heat equation can be re-written as a continuity equation if $\nabla\psi_t=-\frac{\nabla\rho_t}{\hat\rho_t}$ \\
        The gradient of the entropy is $\text{grad}_\W \cH(\rho_t)=\nabla \log \rho_t$

        Hence, we get that the heat flow is the gradient flow of the entropy if \begin{equation*}
        \nabla \psi_t = -\frac{\nabla\rho_t}{\hat\rho_t} = -\nabla \log \rho_t;
        \end{equation*}
        which gives
        \begin{equation}\label{eq: condition}
            \frac{\nabla\rho_t}{\hat\rho_t} = \nabla \log \rho_t
        \end{equation}
        i.e. precisely that $\hat\rho_t$ is the logarithmic mean
    \end{block}
\end{frame}

% other examples of what this theoreitical framework allows us to get     

\end{document}
